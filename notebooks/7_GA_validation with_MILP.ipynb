{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook compares the GA score vs the one of the MILP to validate that the GA can achieve the global optimum.\n",
    "\n",
    "The validation is done without the network metrics of system accessibility and inter-station proximity. \n",
    "\n",
    "Two experiments are done:\n",
    "\n",
    "1. GA vs MILP score across multiple weights\n",
    "2. GA vs MILP score across multiple numbers of stations \n",
    "\n",
    "---\n",
    "Author: Jordi Grau Escolano\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 ['10-19: 0.03489456438290895', '20-29: 0.016417594502611852', '30-39: 0.02739160248198541', '40-49: 0.02879483074285447', '50-59: 0.009736149035489139', '60-69: 0.051067313533762274', '70+: 0.04082567914897312', 'altitude: 0.014291755135009458', 'bike_lane_kms: 0.01878993691399383', 'bus_lines: 0.002382088243706499', 'cars: 0.047129902387407524', 'education_college: 0.0018112182675607538', 'education_primary: 0.006427664613757233', 'education_secondary: 0.02608060585864848', 'f: 0.047893240822790756', 'has_bike_lane: 0.043648998762104485', 'household_avg_m2: 0.04948272243559799', 'income_2022_pers: 0.023182510073711356', 'm: 0.01362975245090035', 'metro_lines: 0.02047134870711412', 'motos: 0.03149095479181811', 'n_civic: 0.00029084587198059247', 'n_culture: 0.028583405964349422', 'n_economic_retail: 0.0519786134913751', 'n_education: 0.037229659555487585', 'n_green: 0.010466209676175906', 'n_health_care: 0.014796615249291043', 'n_industrial: 0.04067356745269744', 'n_recreation: 0.04225112853372472', 'n_sport: 0.00392652790269187', 'n_superpois: 0.03839627460322039', 'n_tourism: 0.007422375907383085', 'n_worship: 0.04294975729720915', 'non_spanish_population: 0.04855444202654426', 'pois_entropy: 0.0038998776768160762', 'pois_total: 0.040622245326138645', 'population: 0.010322274091202871', 'tram_lines: 0.01713491068998721', 'unemployment_percentage: 0.0046608353910184185']\n",
      "1 ['10-19: 0.0032612055617122315', '20-29: 0.04863296820872029', '30-39: 0.03375064921666416', '40-49: 0.039755968036516875', '50-59: 0.00115575228152319', '60-69: 0.05445727781306885', '70+: 0.046738941822751796', 'altitude: 0.016402970081622125', 'bike_lane_kms: 0.025606843540507605', 'bus_lines: 0.01635156727161184', 'cars: 0.010208879444288221', 'education_college: 0.033612723199375895', 'education_primary: 0.05337951046648064', 'education_secondary: 0.04109907467982831', 'f: 0.008759938284627306', 'has_bike_lane: 0.02057002373558768', 'household_avg_m2: 0.011922149171916585', 'income_2022_pers: 0.021029207259567077', 'm: 0.008758584019256013', 'metro_lines: 0.007832125744276898', 'motos: 0.010297565617046667', 'n_civic: 0.05327683103725944', 'n_culture: 0.01121103224464191', 'n_economic_retail: 0.03411166379034832', 'n_education: 0.04538887698851162', 'n_green: 0.0036524349798958155', 'n_health_care: 0.04408507178931226', 'n_industrial: 0.009574373903687242', 'n_recreation: 0.0332621476986929', 'n_sport: 0.0026080393186913783', 'n_superpois: 0.017103070575665905', 'n_tourism: 0.028872588118694864', 'n_worship: 0.054217092210294304', 'non_spanish_population: 0.017082210589352293', 'pois_entropy: 0.03841745491149627', 'pois_total: 0.005483970943463525', 'population: 0.02425230533304879', 'tram_lines: 0.03435354635727724', 'unemployment_percentage: 0.029463363752715594']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>N_stations</th>\n",
       "      <th colspan=\"6\" halign=\"left\">60</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>score_lp</th>\n",
       "      <th>score_ga</th>\n",
       "      <th>accuracy_ga</th>\n",
       "      <th>accuracy_bool</th>\n",
       "      <th>time_lp</th>\n",
       "      <th>time_ga</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>experiment_idx</th>\n",
       "      <th>weights</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <td>53.424</td>\n",
       "      <td>53.107</td>\n",
       "      <td>0.994</td>\n",
       "      <td>True</td>\n",
       "      <td>5.858</td>\n",
       "      <td>33.803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <th>2</th>\n",
       "      <td>53.349</td>\n",
       "      <td>53.169</td>\n",
       "      <td>0.997</td>\n",
       "      <td>True</td>\n",
       "      <td>5.843</td>\n",
       "      <td>28.252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <th>3</th>\n",
       "      <td>51.700</td>\n",
       "      <td>51.200</td>\n",
       "      <td>0.990</td>\n",
       "      <td>True</td>\n",
       "      <td>5.805</td>\n",
       "      <td>31.718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <th>4</th>\n",
       "      <td>53.662</td>\n",
       "      <td>53.585</td>\n",
       "      <td>0.999</td>\n",
       "      <td>True</td>\n",
       "      <td>5.837</td>\n",
       "      <td>36.653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <th>5</th>\n",
       "      <td>53.625</td>\n",
       "      <td>53.355</td>\n",
       "      <td>0.995</td>\n",
       "      <td>True</td>\n",
       "      <td>5.804</td>\n",
       "      <td>33.019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "N_stations                   60                                             \\\n",
       "                       score_lp score_ga accuracy_ga accuracy_bool time_lp   \n",
       "experiment_idx weights                                                       \n",
       "1              1         53.424   53.107       0.994          True   5.858   \n",
       "2              2         53.349   53.169       0.997          True   5.843   \n",
       "3              3         51.700   51.200       0.990          True   5.805   \n",
       "4              4         53.662   53.585       0.999          True   5.837   \n",
       "5              5         53.625   53.355       0.995          True   5.804   \n",
       "\n",
       "N_stations                      \n",
       "                       time_ga  \n",
       "experiment_idx weights          \n",
       "1              1        33.803  \n",
       "2              2        28.252  \n",
       "3              3        31.718  \n",
       "4              4        36.653  \n",
       "5              5        33.019  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import ast\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "project_root = Path().resolve().parents[0]\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from paths import *\n",
    "\n",
    "ROOT = '..'\n",
    "\n",
    "# Load data\n",
    "file_ga_w = pd.read_csv(f'{ROOT}/{PR_EXP}/GA_MILP_weights/GA_results.csv')\n",
    "file_milp_w = pd.read_csv(f'{ROOT}/{PR_EXP}/GA_MILP_weights/MILP_results.csv')\n",
    "\n",
    "file_ga_s = pd.read_csv(f'{ROOT}/{PR_EXP}/GA_MILP_stations/GA_results.csv')\n",
    "file_milp_s = pd.read_csv(f'{ROOT}/{PR_EXP}/GA_MILP_stations/MILP_results.csv')\n",
    "\n",
    "\n",
    "def prepare_data(file_ga, file_lp):\n",
    "\n",
    "    df_ga = pd.DataFrame(file_ga)\n",
    "    df_milp = pd.DataFrame(file_lp)\n",
    "    df_ga.rename(columns={\n",
    "        'minutes_to_complete': 'time', \n",
    "        'best_score': 'score'}, inplace=True)\n",
    "    df_milp.rename(columns={\n",
    "        'minutes_to_complete': 'time',\n",
    "        'best_score': 'score'}, inplace=True)\n",
    "\n",
    "    # # Parse weights\n",
    "    # def parse_weights(weights_str):\n",
    "    #     weights = ast.literal_eval(weights_str)\n",
    "    #     return {k.strip(): float(v) for k, v in [item.split(':') for item in weights]}\n",
    "    \n",
    "    # Parse weights\n",
    "    # df_ga['weights'] = df_ga['weights'].map(parse_weights)\n",
    "    # df_milp['weights'] = df_milp['weights'].map(parse_weights)\n",
    "\n",
    "    # Weights are too long to display, so we'll create a weight_to_idx dictionary\n",
    "    weight_to_idx = {weights: experiment_idx for idx, (weights, experiment_idx) in enumerate(df_ga[['weights', 'experiment_idx']].values)}\n",
    "\n",
    "    # Map weights to indices\n",
    "    df_ga['weights'] = df_ga['weights'].map(weight_to_idx)\n",
    "    df_milp['weights'] = df_milp['weights'].map(weight_to_idx)\n",
    "\n",
    "    # Merge data\n",
    "    df = pd.merge(df_ga, df_milp, on=['experiment_idx', 'N_stations', 'weights'], suffixes=('_ga', '_lp'), how='outer')\n",
    "\n",
    "    # Calculate GA accuracy when compared with linear programming\n",
    "    df['accuracy_ga'] = df['score_ga'] / df['score_lp']\n",
    "    df['accuracy_bool'] = df['accuracy_ga'] > 0.95\n",
    "    df_table = df[['experiment_idx', 'N_stations', 'weights', 'score_ga', 'score_lp', 'accuracy_ga', 'accuracy_bool', 'time_ga', 'time_lp']]\n",
    "    df_table.sort_values('weights')\n",
    "\n",
    "    # Create pivot table\n",
    "    pivot_df = pd.pivot_table(\n",
    "        df_table.round(3),\n",
    "        index=['experiment_idx', 'weights'],\n",
    "        columns='N_stations',\n",
    "        values=['score_ga', 'score_lp', 'accuracy_ga', 'accuracy_bool', 'time_ga', 'time_lp'],\n",
    "        aggfunc='first')\n",
    "\n",
    "    # Swap the levels of the columns\n",
    "    pivot_df = pivot_df.swaplevel(0, 1, axis=1).sort_index(axis=1)\n",
    "\n",
    "    # Reorder the metrics within each N_stations group\n",
    "    metric_order = ['score_lp', 'score_ga', 'accuracy_ga', 'accuracy_bool', 'time_lp', 'time_ga']\n",
    "    ordered_columns = []\n",
    "    for n_stations in (pivot_df.columns.levels[0]):\n",
    "        for metric in metric_order:\n",
    "            ordered_columns.append((n_stations, metric))\n",
    "    pivot_df = pivot_df.reindex(columns=ordered_columns)\n",
    "\n",
    "    # Fillna\n",
    "    pivot_df = pivot_df.fillna(0)\n",
    "\n",
    "    return pivot_df, weight_to_idx\n",
    "\n",
    "\n",
    "# Prepare data for weights and stations\n",
    "df_weights, weight_to_idx_w = prepare_data(file_ga_w, file_milp_w)\n",
    "df_stations, weight_to_idx_s = prepare_data(file_ga_s, file_milp_s)\n",
    "\n",
    "# Weights have an index to ease the visualization of the results\n",
    "for key, value in weight_to_idx_w.items():\n",
    "    if value <= 2:\n",
    "        print(value, key)\n",
    "\n",
    "df_weights.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. GA vs MILP score comparison  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary_statistics(pivot_df):\n",
    "    \"\"\"Generate summary statistics for each N_stations\"\"\"\n",
    "    summaries = {}\n",
    "\n",
    "    # Drop columns with all zeros\n",
    "    columns = pivot_df.loc[:, (pivot_df != 0).any(axis=0)].columns\n",
    "    n_stations = [col[0] for col in columns]\n",
    "    n_stations_list = sorted(set(n_stations))\n",
    "\n",
    "    # For each N_stations group\n",
    "    for n in n_stations_list: \n",
    "        # Get the data for this N_stations\n",
    "        station_data = pivot_df[n].dropna()\n",
    "        round_to = 2\n",
    "\n",
    "        # Calculate statistics\n",
    "        # Use only valid GA and MILP solutions\n",
    "        valid_solutions = station_data[\n",
    "            (station_data['accuracy_ga'] > 0) &\n",
    "            (station_data['score_lp'] > 0)]\n",
    "        \n",
    "        # If there are valid solutions, compute averages and std.\n",
    "        if len(valid_solutions) > 0:\n",
    "            avg_ga_score = valid_solutions['score_ga'].mean()\n",
    "            std_ga_score = valid_solutions['score_ga'].std()\n",
    "            avg_acc = valid_solutions['accuracy_ga'].mean()\n",
    "            std_acc = valid_solutions['accuracy_ga'].std()\n",
    "            avg_ga_time = valid_solutions['time_ga'].mean()\n",
    "            std_ga_time = valid_solutions['time_ga'].std()\n",
    "            \n",
    "            avg_lp_score = valid_solutions['score_lp'].mean()\n",
    "            std_lp_score = valid_solutions['score_lp'].std()\n",
    "            avg_lp_time = valid_solutions['time_lp'].mean()\n",
    "            std_lp_time = valid_solutions['time_lp'].std()\n",
    "\n",
    "        else:\n",
    "            avg_ga_score = std_ga_score = avg_acc = std_acc = avg_ga_time = std_ga_time = 0\n",
    "        \n",
    "        statistics = {\n",
    "            f\"N={n}\": {\n",
    "                \"Number of experiments\": len(valid_solutions),\n",
    "                \"Avg LP score (Std)\": f\"{avg_lp_score:.{round_to}f} ({std_lp_score:.{round_to}f})\",\n",
    "                \"Avg GA score (Std)\": f\"{avg_ga_score:.{round_to}f} ({std_ga_score:.{round_to}f})\",\n",
    "                \"Accuracy\": f\"{avg_acc:.{round_to}f} ({std_acc:.{round_to}f})\",\n",
    "                \"Avg LP time (Std)\": f\"{avg_lp_time:.{round_to}f} ({std_lp_time:.{round_to}f})\",\n",
    "                \"Avg GA time (Std)\": f\"{avg_ga_time:.{round_to}f} ({std_ga_time:.{round_to}f})\",\n",
    "                \"Good solutions\": valid_solutions['accuracy_bool'].sum(),\n",
    "                \"Bad solutions\": len(valid_solutions) - valid_solutions['accuracy_bool'].sum()\n",
    "            }\n",
    "        }\n",
    "        summaries.update(statistics)\n",
    "    \n",
    "    # Convert to DataFrame for nice display\n",
    "    summary_df = pd.DataFrame(summaries).T\n",
    "\n",
    "    # Perform power analysis and statistical tests between LP and GA scores for each N_stations\n",
    "    # Threshold for statistical significance in percentage\n",
    "    threshold_percent = 0.5\n",
    "    for n in n_stations_list:\n",
    "        # Get the data for this N_stations\n",
    "        station_data = pivot_df[n].dropna()\n",
    "        \n",
    "        # Only compare where GA produced valid solutions\n",
    "        valid_solutions = station_data[\n",
    "            (station_data['score_ga'] != 0) &\n",
    "            (station_data['score_lp'] != 0)]\n",
    "        \n",
    "        if len(valid_solutions) > 0:\n",
    "            lp_scores = valid_solutions['score_lp']\n",
    "            ga_scores = valid_solutions['score_ga']\n",
    "            \n",
    "            # Compute the percentage difference:\n",
    "            # If GA is 99% of LP, then (LP - GA) / LP * 100 = 1.\n",
    "            differences_percent = (lp_scores - ga_scores) / lp_scores * 100\n",
    "\n",
    "            # Test normality of these differences.\n",
    "            _, p_normality = stats.shapiro(differences_percent)\n",
    "            is_normal = p_normality > 0.05\n",
    "            print(f\"N={n}: Shapiro p={p_normality:.3f} ({'normal' if is_normal else 'not normal'})\")\n",
    "            \n",
    "            if is_normal:\n",
    "                # When normal, perform a one-sample t-test:\n",
    "                # H0: mean(differences) >= threshold_percent \n",
    "                # H1: mean(differences) < threshold_percent\n",
    "                t_stat, p_two_tailed = stats.ttest_1samp(differences_percent, popmean=threshold_percent)\n",
    "                # Convert the two-tailed p-value into a one-tailed p-value.\n",
    "                if t_stat < 0:\n",
    "                    p_one_tailed = p_two_tailed / 2\n",
    "                else:\n",
    "                    p_one_tailed = 1 - (p_two_tailed / 2)\n",
    "                print(\"Test: One-sample t-test on percentage differences\")\n",
    "                print(f\"t-statistic = {t_stat:.4f}, one-tailed p-value = {p_one_tailed:.4f}\")\n",
    "            else:\n",
    "                # When non-normal, use the Wilcoxon signed-rank test.\n",
    "                # Adjust the differences by subtracting the threshold so that H0: median(adjusted_diff)=0.\n",
    "                adjusted_diff = differences_percent - threshold_percent\n",
    "                wilcoxon_stat, p_two_tailed = stats.wilcoxon(adjusted_diff)\n",
    "                if np.median(adjusted_diff) < 0:\n",
    "                    p_one_tailed = p_two_tailed / 2\n",
    "                else:\n",
    "                    p_one_tailed = 1 - (p_two_tailed / 2)\n",
    "                print(\"Test: Wilcoxon signed-rank test on adjusted percentage differences\")\n",
    "                print(f\"Wilcoxon statistic = {wilcoxon_stat:.1f}, one-tailed p-value = {p_one_tailed:.4f}\")\n",
    "\n",
    "            \n",
    "            # ------------------------\n",
    "            # Perform a power analysis for this experiment\n",
    "            # ------------------------\n",
    "            observed_mean = differences_percent.mean()\n",
    "\n",
    "            # Only do the power analysis if the observed mean difference is less than the threshold.\n",
    "            threshold_percent = 0.5\n",
    "            if observed_mean < threshold_percent:\n",
    "                \n",
    "                # Compute the effect size as the difference between the threshold and the observed mean difference.\n",
    "                # Also compute the variability.\n",
    "                delta_effect = threshold_percent - observed_mean\n",
    "                sigma = differences_percent.std()\n",
    "                print(f\"threshold_percent = {threshold_percent:.2f}, observed_mean = {observed_mean:.2f}, delta_effect = {delta_effect:.2f}, sigma = {sigma:.2f}\")\n",
    "\n",
    "                # Set the significance level for the test. We want less than 0.1% chance of Type I error.\n",
    "                alpha = 0.001\n",
    "                # Set the desired power (Error type II is 1 - desired_power). We want a 99% chance of detecting the effect if it exists.\n",
    "                desired_power = 0.9999\n",
    "                print(f\"desired_power = {desired_power:.4f}, alpha = {alpha:.4f}\")\n",
    "\n",
    "                # Calculate the critical z-value for the chosen significance level. As it is one-sided -> 1 - alpha.\n",
    "                z_alpha = stats.norm.ppf(1 - alpha)\n",
    "\n",
    "                # Calculate the critical z-value corresponding to the desired power.\n",
    "                # This is the z-value such that the area to the left equals the desired power.\n",
    "                z_beta = stats.norm.ppf(desired_power)\n",
    "\n",
    "                # Compute the required sample size (n) using the standard formula for power analysis in a paired design.\n",
    "                # This formula approximates how many paired observations (valid experiments) are needed.\n",
    "                n_estimate = ((z_alpha + z_beta) * sigma / delta_effect) ** 2\n",
    "\n",
    "                # Since the sample size must be an integer, round the computed value up to the nearest whole number.\n",
    "                required_n = int(np.ceil(n_estimate))\n",
    "\n",
    "                # Print the estimated required sample size based on the desired power and significance level.\n",
    "                print(f\"Estimated required sample size (power={desired_power}): {required_n}\")\n",
    "            else:\n",
    "                # If the observed mean difference is not below the threshold, the data do not support the alternative hypothesis.\n",
    "                # In this case, performing a power analysis for detecting a difference below the threshold is not applicable.\n",
    "                print(\"Observed mean difference is not below the threshold; power analysis not applicable.\")\n",
    "\n",
    "            # Print the final one-tailed p-value from the hypothesis test and the number of valid experiments used.\n",
    "            # This gives an idea whether the test is significant relative to the significance level (alpha)\n",
    "            # and how many paired observations (experiments) you had.\n",
    "            print(f\"(p = {p_one_tailed:.4f}), {'<' if p_one_tailed < alpha else '>'} than significance level, n = {len(valid_solutions)}\\n\")\n",
    "            \n",
    "    cols_order = ['Avg LP score (Std)', 'Avg GA score (Std)', 'Avg LP time (Std)', 'Avg GA time (Std)', 'Accuracy', 'Bad solutions', 'Good solutions', 'Number of experiments']\n",
    "    return summary_df[cols_order]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At least n paired samples are needed to be 99.99% confident of detecting that the GA algorithm's performance is better than the threshold (0.5%), with only a 0.01% chance of falsely claiming it's better when it's not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=60: Shapiro p=0.017 (not normal)\n",
      "Test: Wilcoxon signed-rank test on adjusted percentage differences\n",
      "Wilcoxon statistic = 965.0, one-tailed p-value = 0.0000\n",
      "threshold_percent = 0.50, observed_mean = 0.35, delta_effect = 0.15, sigma = 0.19\n",
      "desired_power = 0.9999, alpha = 0.0010\n",
      "Estimated required sample size (power=0.9999): 76\n",
      "(p = 0.0000), < than significance level, n = 118\n",
      "\n",
      "\n",
      "Summary Statistics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Avg LP score (Std)</th>\n",
       "      <th>Avg GA score (Std)</th>\n",
       "      <th>Avg LP time (Std)</th>\n",
       "      <th>Avg GA time (Std)</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Bad solutions</th>\n",
       "      <th>Good solutions</th>\n",
       "      <th>Number of experiments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>N=60</th>\n",
       "      <td>53.48 (0.51)</td>\n",
       "      <td>53.29 (0.54)</td>\n",
       "      <td>5.92 (0.07)</td>\n",
       "      <td>33.60 (10.25)</td>\n",
       "      <td>1.00 (0.00)</td>\n",
       "      <td>0</td>\n",
       "      <td>118</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Avg LP score (Std) Avg GA score (Std) Avg LP time (Std)  \\\n",
       "N=60       53.48 (0.51)       53.29 (0.54)       5.92 (0.07)   \n",
       "\n",
       "     Avg GA time (Std)     Accuracy Bad solutions Good solutions  \\\n",
       "N=60     33.60 (10.25)  1.00 (0.00)             0            118   \n",
       "\n",
       "     Number of experiments  \n",
       "N=60                   118  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Statistics for 60 stations\n",
    "summary_stats = generate_summary_statistics(df_weights[[60]])\n",
    "print(\"\\nSummary Statistics:\")\n",
    "summary_stats\n",
    "\n",
    "# This is a “good” statistical conclusion in the sense that there is strong evidence\n",
    "# against the null hypothesis, which states that the GA score is not within\n",
    "# the 5% margin of the LP score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=30: Shapiro p=0.000 (not normal)\n",
      "Test: Wilcoxon signed-rank test on adjusted percentage differences\n",
      "Wilcoxon statistic = 24.0, one-tailed p-value = 0.0002\n",
      "threshold_percent = 0.50, observed_mean = 0.23, delta_effect = 0.27, sigma = 0.23\n",
      "desired_power = 0.9999, alpha = 0.0010\n",
      "Estimated required sample size (power=0.9999): 33\n",
      "(p = 0.0002), < than significance level, n = 24\n",
      "\n",
      "N=60: Shapiro p=0.066 (normal)\n",
      "Test: One-sample t-test on percentage differences\n",
      "t-statistic = -5.0070, one-tailed p-value = 0.0000\n",
      "threshold_percent = 0.50, observed_mean = 0.33, delta_effect = 0.17, sigma = 0.17\n",
      "desired_power = 0.9999, alpha = 0.0010\n",
      "Estimated required sample size (power=0.9999): 45\n",
      "(p = 0.0000), < than significance level, n = 24\n",
      "\n",
      "N=90: Shapiro p=0.858 (normal)\n",
      "Test: One-sample t-test on percentage differences\n",
      "t-statistic = -4.9317, one-tailed p-value = 0.0000\n",
      "threshold_percent = 0.50, observed_mean = 0.34, delta_effect = 0.16, sigma = 0.16\n",
      "desired_power = 0.9999, alpha = 0.0010\n",
      "Estimated required sample size (power=0.9999): 46\n",
      "(p = 0.0000), < than significance level, n = 24\n",
      "\n",
      "N=120: Shapiro p=0.068 (normal)\n",
      "Test: One-sample t-test on percentage differences\n",
      "t-statistic = -5.9211, one-tailed p-value = 0.0000\n",
      "threshold_percent = 0.50, observed_mean = 0.30, delta_effect = 0.20, sigma = 0.17\n",
      "desired_power = 0.9999, alpha = 0.0010\n",
      "Estimated required sample size (power=0.9999): 32\n",
      "(p = 0.0000), < than significance level, n = 24\n",
      "\n",
      "\n",
      "Summary Statistics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Avg LP score (Std)</th>\n",
       "      <th>Avg GA score (Std)</th>\n",
       "      <th>Avg LP time (Std)</th>\n",
       "      <th>Avg GA time (Std)</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Bad solutions</th>\n",
       "      <th>Good solutions</th>\n",
       "      <th>Number of experiments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>N=30</th>\n",
       "      <td>27.73 (0.10)</td>\n",
       "      <td>27.67 (0.13)</td>\n",
       "      <td>5.92 (0.08)</td>\n",
       "      <td>10.51 (2.76)</td>\n",
       "      <td>1.00 (0.00)</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N=60</th>\n",
       "      <td>53.44 (0.43)</td>\n",
       "      <td>53.26 (0.46)</td>\n",
       "      <td>5.93 (0.11)</td>\n",
       "      <td>21.45 (4.69)</td>\n",
       "      <td>1.00 (0.00)</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N=90</th>\n",
       "      <td>77.77 (0.77)</td>\n",
       "      <td>77.50 (0.76)</td>\n",
       "      <td>5.94 (0.06)</td>\n",
       "      <td>38.01 (11.86)</td>\n",
       "      <td>1.00 (0.00)</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N=120</th>\n",
       "      <td>100.89 (1.03)</td>\n",
       "      <td>100.60 (1.03)</td>\n",
       "      <td>5.96 (0.10)</td>\n",
       "      <td>65.66 (20.23)</td>\n",
       "      <td>1.00 (0.00)</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Avg LP score (Std) Avg GA score (Std) Avg LP time (Std)  \\\n",
       "N=30        27.73 (0.10)       27.67 (0.13)       5.92 (0.08)   \n",
       "N=60        53.44 (0.43)       53.26 (0.46)       5.93 (0.11)   \n",
       "N=90        77.77 (0.77)       77.50 (0.76)       5.94 (0.06)   \n",
       "N=120      100.89 (1.03)      100.60 (1.03)       5.96 (0.10)   \n",
       "\n",
       "      Avg GA time (Std)     Accuracy Bad solutions Good solutions  \\\n",
       "N=30       10.51 (2.76)  1.00 (0.00)             0             24   \n",
       "N=60       21.45 (4.69)  1.00 (0.00)             0             24   \n",
       "N=90      38.01 (11.86)  1.00 (0.00)             0             24   \n",
       "N=120     65.66 (20.23)  1.00 (0.00)             0             24   \n",
       "\n",
       "      Number of experiments  \n",
       "N=30                     24  \n",
       "N=60                     24  \n",
       "N=90                     24  \n",
       "N=120                    24  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Statistics for 60 stations\n",
    "summary_stats = generate_summary_statistics(df_stations) \n",
    "# 24 experiments leads to a power analysis of 33[30 and 120 stations] and 46 [60 and 90 stations] experiments\n",
    "print(\"\\nSummary Statistics:\")\n",
    "summary_stats"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
